=========================================
Partitioning large data sets (50m+ reads)
=========================================

We've successfully used khmer to partition extremely large data sets.
These are challenging because of the data size, the memory
constraints, and the time involved.  You'll probably need 60-80gb of
RAM to follow this process; we usually just rent EC2 machines at
http://aws.amazon.com/.

Here's a rough guide.

Step 1a: Load the reads into a hashtable and tagset
==================================================

Load all of the k-mers in all of the reads in memory.

This builds the basic data structure (a fixed-size hash table), which
we can traverse as a probabilistic de Bruijn graph; and a set of
k-mers that are tagged at a minimum density within that graph (the
tag set).  In terms of memory consumption, the hashtable is fixed size
and the tag set is linear with the number of reads (one tagged k-mer
per read).

Implemented in do-th-subset-save.py (first part).

Make the hashtable as big as possible.  Use large K.

Note: hashtable occupancy should be less than 50%, rule of thumb.

Optional: save the hashtable and tagset.  See ht.save(filename) and
ht.save_tagset(filename).

Example: ::

  %% python ./scripts/do-th-subset-save.py data/25k.fa

Step 1b: Do subset partitioning
==============================

Break the set of all tags into smaller subsets (1-5m in size is a good
low-memory choice) and do a tabu search from each tag to all possible
connected tags (stopping at the first connected tag in each
path). Then label all connected tags with a partition ID. These
subsets can be evaluated in parallel, then saved to disk when finished.

Implemented in do-th-subset-save.py (second part).

The smaller the subset size, the less memory used and the more threads
can be used to partition subsets in parallel.  We would suggest
using a subset size of 1m with 8 threads, which is small enough to be
evaluated and saved quickly, without using much memory.

After subset partitioning, neither the hashtable nor the original tagset
is needed.

Step 2: Merge subsets
=====================

Do a pairwise merge of as many subsets as possible.  This is memory-
and CPU- and disk-intensive.  Keep the number of parallel threads as
high as possible without running out of memory.

Implemented in do-subset-merge.py.

Repeat step #2 until you're sick of doing merges.  If you run out of
memory, you may not be able to move on to step 3, but that hasn't
happened to us -- if you can partition (step 1), you can probably
do step 2.

Step 3: Output the reads with their partitions
==============================================

Next, load/merge all the merged subsets (either as in step 2), and run
through the entire original read file. Output any read that has been
assigned a partition.

Implemented in do-th-subset-load.py.

Step 4: Group the sequences into separate groups, based on partition size
=========================================================================

Finally, we want to produce some files feed to the assembler.  While
we *could* assemble each partition individually, in practice there
will 10s of thousands to millions of partitions in a single large data
set.  So instead we want to group them.

The strategy we use is to go through the partitions, counting the
number of reads in each; then order the partitions by size, smallest
to largest; then output all the reads into a series of group files,
each of a particular minimum size (containing a set of reads).
(There's no particular logic as to why to do it this way; you could
just as easily randomly assign partitions to groups.)

Implemented in extract-partitions.py.
